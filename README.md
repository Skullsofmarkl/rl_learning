# Сравнительный анализ алгоритмов обучения с подкреплением для автономного вождения

Этот проект представляет собой комплексный сравнительный анализ четырех популярных алгоритмов обучения с подкреплением (RL) для задачи автономного вождения в среде highway-env.

## Алгоритмы

- **DQN** (Deep Q-Network) - алгоритм на основе Q-обучения с глубокими нейронными сетями
- **PPO** (Proximal Policy Optimization) - алгоритм оптимизации политики с ограничениями
- **SAC** (Soft Actor-Critic) - алгоритм актор-критик с максимальной энтропией
- **A2C** (Advantage Actor-Critic) - классический алгоритм актор-критик

## Структура проекта

```
rl_learning/
├── README.md                 # Этот файл
├── requirements.txt          # Зависимости Python
├── main.py                  # Основной скрипт для запуска анализа
├── agents/                  # Модули с агентами RL
│   ├── __init__.py
│   ├── dqn_agent.py        # DQN агент
│   ├── ppo_agent.py        # PPO агент
│   ├── sac_agent.py        # SAC агент
│   └── a2c_agent.py        # A2C агент
├── networks/                # Нейронные сети
│   ├── __init__.py
│   ├── dqn_networks.py     # Сети для DQN
│   ├── ppo_networks.py     # Сети для PPO
│   ├── sac_networks.py     # Сети для SAC
│   └── a2c_networks.py     # Сети для A2C
├── buffers/                 # Буферы опыта
│   ├── __init__.py
│   └── replay_buffers.py    # Различные типы буферов
├── environment/              # Настройки среды
│   ├── __init__.py
│   └── highway_env.py       # Конфигурация highway-env
├── training/                 # Функции обучения
│   ├── __init__.py
│   ├── training_functions.py # Функции обучения для всех алгоритмов
│   └── evaluation.py        # Функции оценки агентов
├── analysis/                 # Анализ и визуализация
│   ├── __init__.py
│   ├── comparison.py        # Сравнительный анализ
│   ├── visualization.py     # Построение графиков
│   └── metrics.py           # Расчет метрик
└── utils/                   # Вспомогательные функции
    ├── __init__.py
    └── helpers.py           # Общие утилиты
```

## Быстрый старт

### Установка зависимостей

```bash
pip install -r requirements.txt
```

### Запуск полного анализа

```bash
python main.py
```

### Демонстрация отдельного алгоритма

```python
from main import demonstrate_algorithm

# Демонстрация DQN
results = demonstrate_algorithm('DQN', episodes=50, max_steps=1000)

# Демонстрация PPO
results = demonstrate_algorithm('PPO', episodes=50, max_steps=1000)

# Демонстрация SAC
results = demonstrate_algorithm('SAC', episodes=50, max_steps=1000)

# Демонстрация A2C
results = demonstrate_algorithm('A2C', episodes=50, max_steps=1000)
```

## Возможности

- **Автоматическое обучение** всех 4 алгоритмов с одинаковыми параметрами
- **Сравнительный анализ** производительности и метрик
- **Визуализация результатов** с помощью графиков и диаграмм
- **Запись видео** лучших эпизодов для каждого алгоритма
- **Сохранение моделей** и результатов анализа
- **Детальная отчетность** с метриками и рекомендациями

## Настройки

Основные параметры можно изменить в файле `main.py`:

- `episodes` - количество эпизодов для обучения
- `max_steps` - максимальное количество шагов в эпизоде
- `save_models` - сохранять ли обученные модели

## Метрики анализа

- **Средняя награда** - основной показатель производительности
- **Успешность** - процент успешных эпизодов
- **Стабильность** - стандартное отклонение наград
- **Скорость обучения** - динамика улучшения наград
- **Обобщающая способность** - разница между обучением и оценкой

## Результаты

После выполнения анализа создаются:

1. **Графики сравнения** - награды, успешность, стабильность
2. **Видео эпизодов** - лучшие результаты каждого алгоритма
3. **Сохраненные модели** - обученные агенты
4. **Отчеты** - CSV и JSON файлы с детальными результатами
5. **Рекомендации** - по выбору алгоритма для конкретных задач

## Устранение неполадок

### Проблемы с CUDA
Если возникают проблемы с GPU, код автоматически переключится на CPU.

### Проблемы с highway-env
Убедитесь, что установлена последняя версия:
```bash
pip install --upgrade highway-env
```

### Нехватка памяти
Уменьшите количество эпизодов или максимальное количество шагов в `main.py`.

## Теоретическая база

- **DQN**: [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236)
- **PPO**: [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)
- **SAC**: [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning](https://arxiv.org/abs/1801.01290)
- **A2C**: [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783)

