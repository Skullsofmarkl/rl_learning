"""
Нейронные сети для DQN (Deep Q-Network) алгоритма.

DQN использует одну нейронную сеть для оценки Q-значений:
- Q-сеть принимает состояние и выдает Q-значения для всех возможных действий
- Q(s,a) - это ожидаемая награда при выполнении действия 'a' в состоянии 's'

Архитектура:
- Полносвязные слои с ReLU активацией
- Xavier инициализация весов для стабильного обучения
- Глубокая архитектура для извлечения сложных признаков из состояния

Особенности DQN:
- Off-policy алгоритм (может обучаться на опыте других политик)
- Использует Experience Replay для стабилизации обучения
- Target Network для стабильных целевых значений
- Epsilon-greedy стратегия для баланса exploration/exploitation
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class DQNNetwork(nn.Module):
    """
    Q-сеть для DQN алгоритма.
    
    Q-сеть (Q-Network) - это нейронная сеть, которая оценивает Q-значения действий.
    Она принимает состояние среды и выдает Q-значения для каждого возможного действия.
    Q(s,a) представляет ожидаемую награду при выполнении действия 'a' в состоянии 's'.
    
    Архитектура:
    - Входной слой: state_size -> 256 нейронов
    - Скрытые слои: 256 -> 256 -> 128 нейронов
    - Выходной слой: 128 -> action_size (Q-значения для каждого действия)
    
    Функция активации: ReLU (Rectified Linear Unit)
    Инициализация: Xavier uniform для стабильного обучения
    
    Использование:
    - Выбор действия: argmax(Q(s,a)) для exploitation
    - Epsilon-greedy: случайное действие с вероятностью epsilon для exploration
    - Обучение: минимизация TD-ошибки между предсказанными и целевыми Q-значениями
    """
    
    def __init__(self, state_size, action_size):
        """
        Инициализация Q-сети.
        
        Args:
            state_size (int): размерность входного состояния
            action_size (int): количество возможных действий
        """
        super(DQNNetwork, self).__init__()
        
        # Определяем архитектуру сети
        # Полносвязные слои с постепенным уменьшением размерности
        self.fc1 = nn.Linear(state_size, 256)      # Первый скрытый слой
        self.fc2 = nn.Linear(256, 256)             # Второй скрытый слой
        self.fc3 = nn.Linear(256, 128)             # Третий скрытый слой
        self.q_head = nn.Linear(128, action_size)  # Выходной слой (Q-значения)
        
        # Инициализируем веса сети для стабильного обучения
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """
        Инициализация весов нейронной сети.
        
        Используется Xavier uniform инициализация:
        - Веса инициализируются из равномерного распределения
        - Дисперсия зависит от размера слоя
        - Помогает избежать проблем с исчезающими/взрывающимися градиентами
        
        Args:
            module: модуль нейронной сети для инициализации
        """
        if isinstance(module, nn.Linear):
            # Xavier uniform инициализация весов
            torch.nn.init.xavier_uniform_(module.weight)
            # Инициализация смещений нулями
            if module.bias is not None:
                torch.nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        """
        Прямой проход через Q-сеть.
        
        Args:
            x (torch.Tensor): входное состояние (batch_size, state_size)
            
        Returns:
            torch.Tensor: Q-значения для всех действий (batch_size, action_size)
        """
        # Прямой проход через сеть с ReLU активацией
        x = F.relu(self.fc1(x))    # Первый скрытый слой + ReLU
        x = F.relu(self.fc2(x))    # Второй скрытый слой + ReLU
        x = F.relu(self.fc3(x))    # Третий скрытый слой + ReLU
        return self.q_head(x)      # Выходной слой (Q-значения без активации)
